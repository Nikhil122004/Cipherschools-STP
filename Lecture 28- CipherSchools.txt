Lecture 28- Feature Engineering And Selection


Feature Engineering Methods

Handling Missing Values

Encoding Categorical Variables

Feature Selection

Feature Creation


Handling Missing Values

import pandas as pd
from sklearn.impute import SimpleImputer

# Sample data
data = {
'Featurel' :[1.0,2.0,None,4.0,5.0]
'Feature2':[2.0,None,4.0,5.0,None]
'Feature3':[None,3.0,3.5,4.0,4.5]
}
df = pd.DataFrame(data)

# Handling missing values
imputer = SimpleImputer(strategy='mean')
df_imputed=pd.DataFrame(imputer.fit_transform(df),columns=df.columns)
print("After Imputation: \n", df_imputed)


Encoding categorical Variables

import panda as pd
from sklearn.preprocessing import OneHotEncoder

#sample data
data={
'Color': ['Red','Blue','Green','Blue','Red']
}
df=pd.DataFrame(data)

# Encoding categorical variables
encoder=OneHotEncoder(sparse=False)
encoded_categories=encoder.fit_transform(df[['Color']])
df_encoded=pd.DataFrame(encoded_categories,columns=encoder.get_feature_names_out(['color']))
df=pd.concat([df,df_encoded],axis=1).drop('color',axis=1)
print('After one-hot encoding:\n",df)



Feature Scaling

import pandas as pd
from sklearn.preprocessing import MinMaxS

# Sample data
data ={
'feature1':[10,20,30,40,50]
'feature2':[100,200,300,400,500]
}
df = pd.DataFrame(data)

# Feature scaling
scaler = MinMaxSca1er()
df_scaled = pd.DataFrame(sca1er.fit _ transform(df),columns=df.columns)
print("After Min-Max Scaling: \n",df_scaled)


Feature Creation

import panda as pd
from sklearn.preprocessing import Polynomialfeatures

# Sample data
data ={
'feature1':[1,2,3,4,5]
'feature2':[2,3,4,5,6]
}
df = pd.DataFrame(data)

# Feature creation
poly = PolynomialFeatures(degree=2, include_bias=Fa1se)
poly_features = poly.fit_transform(df)
df_poly = pd.DataFrame( poly_features,columns=poly.get_feature_names_out([' featurel' ,'feature2']))
print("After Creating Polynomial Features:\n",df_poly)



Feature Selection Methods

Variance Thresholding

Correlation Matrix filtering

Domain Knowledge



Variance Thresholding

import panda as pd
from sklearn.feature_selection import VarianceThreshold

#sample data
data={
'Feature1':[1,1,1,1,1], #low variance
'Feature2':[2,3,4,5,6],
'Feature3':[3,4,5,6,7],
'Constant':[1,1,1,1,1] #zero variance
}
df=pd.DataFrame(data)

#variance Thresholding
selector=VarianceThreshold(threshold=0.1)
df_variance_filtered=pd.DataFrame(selector.fit_transform(df),columns=df.columns[selector.get_support()])
print("After variance Thresholding:\n",df_variance_filtered)



Correlation Matrix filtering

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

#sample data
data={
'Feature1':[1,2,3,4,5], 
'Feature2':[2,4,6,8,10],#Highly correlated with feature1
'Feature3':[2,4,6,7,10],#Highly correlated with feature2
'Feature4':[5,6,7,8,9]
}
df=pd.DataFrame(data)


#correlation matrix
corr_matrix=df.corr().abs()

#plot correlation matrix
plt.figure(figsize=(10,8))
sns.heatmao(corr_matrix,annot=True, cmap='coolwarm',fmt='.2f')
plt.show()

#select upper triangle  of correlation matrix
upper=corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(bool))

#find features with correlation greater than 0.9
to_drop=[column for column in upper.columns if any(upper[column]>0.9)]


#drop features
df_corr_fiiltered=df.drop(to_drop,axis=1)
print("After rcorrelation Matrix Filtering:\n",df_corr_filtered)



Domain Knowledge

import pandas as pd

#sample data
data={
'Age':[25,30,35,40,45],
'Salary':[50000,60000,70000,80000,90000],
'Height':[5.5,6.0,5.8,5.9,6.1],
'Weight':[150,160,170,180,190]
}
df=pd.DataFrame(data)

# Based on domain knowledge, we know Age and Salary are important
selected_features_domain = df[[ 'Age','Salary' ]]
print("Se1ected Features based on Domain Knowledge:\n",selected features_domain)















