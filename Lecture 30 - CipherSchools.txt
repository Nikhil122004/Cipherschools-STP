Lecture 30 - Supervised Learning(Linear Regression, Logistic regression,Decision Trees, Random Forests, svms)


Introduction to Supervised Learning

Supervised learning is a fundamental machine leaming technique wherean algorithm is trained on labeled data to make predictions on new.unseen data. This powerful approach has numerous applications, fromspam filtering to medical diagnosis.


Reasons for Underfitting
• Too simple model
• Insufficient training
• Poor feature selection
Reasons for Overfitting
• Too complex model
• Too much training
• Limited training data


Linear Regression

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

#Generating synthetic data
import numpy as np
x=np.random.rand(100, 1) * 10
y=2.5 * X + np.random.randn(100,1)*2

#Splitting data into training and testing sets
X_train, X_test, y_train, y_test ==train_test_split(X,y,test_size=0.2,random_state=42)

#Training the model
model=LinearRegression()
model.fit(X_train,y_train)

#Making preddictions
y_pred=model.predict(X_test)

# Evaluating the model
mse=mean_squared_error(y_test,y_pred)
print(f'Mean Sqaured Error: {mse}')


Logistic Regression: Predicting Binary Outcomes

from sklern.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.logistic_model import LogisticRegression
from sklearn.metrics import accuracy score


#Load the Iris dataset
iris=load_iris()
X=iris.data
y=iris.target

#using only two classes for binary classification
X=X[y!=2]
y=y[y !=2]

#splitting data into training and testing sets
X_train, X_test, y_train, y_test ==train_test_split(X,y,test_size=0.2,random_state=42)


#Training the model
model=LogisticRegression()
model.fit(X_train,y_train)

#Making preddictions
y_pred=model.predict(X_test)

#Evaluating the model
accuracy=accuracy_score(y_test,y_pred)
print(f'Accuracy: {accuracy}')



Decision Trees: Building a Hierarchial Model


from sklern.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score


#Load the Iris dataset
iris=load_iris()
X=iris.data
y=iris.target


#splitting data into training and testing sets
X_train, X_test, y_train, y_test ==train_test_split(X,y,test_size=0.2,random_state=42)


#Training the model
model=DecisionTreeClassifier()
model.fit(X_train,y_train)

#Making preddictions
y_pred=model.predict(X_test)

#Evaluating the model
accuracy=accuracy_score(y_test,y_pred)
print(f'Accuracy: {accuracy}')



Support Vector Machines(SVMs):


from sklern.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

#Load the Iris dataset
iris=load_iris()
X=iris.data
y=iris.target


#using only two classes for binary classification
X=X[y!=2]
y=y[y !=2]


#splitting data into training and testing sets
X_train, X_test, y_train, y_test ==train_test_split(X,y,test_size=0.2,random_state=42)


#Training the model
model=SVC()
model.fit(X_train,y_train)

#Making preddictions
y_pred=model.predict(X_test)

#Evaluating the model
accuracy=accuracy_score(y_test,y_pred)
print(f'Accuracy: {accuracy}')













